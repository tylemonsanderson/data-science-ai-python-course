{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6060f93",
   "metadata": {},
   "source": [
    "# Notebook 9: Scikit-Learn Basics - Your First Machine Learning Models\n",
    "\n",
    "Welcome to your ninth Python notebook! Now you'll take the next step and build your first machine learning models using scikit-learn, the most popular ML library in Python.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand the machine learning workflow\n",
    "- Load and explore real datasets\n",
    "- Build classification and regression models\n",
    "- Evaluate model performance\n",
    "- Apply ML to solve real-world problems\n",
    "\n",
    "**Prerequisites:** You should have completed notebooks 1-8, especially NumPy, Pandas, and Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523ca5e",
   "metadata": {},
   "source": [
    "## Essential Imports for Machine Learning\n",
    "\n",
    "Every ML project starts with these imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5ba0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"Big 4\" for machine learning\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, classification_report\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üöÄ All libraries imported successfully!\")\n",
    "print(\"Ready to build your first ML models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a281f8",
   "metadata": {},
   "source": [
    "## The Machine Learning Workflow\n",
    "\n",
    "Every ML project follows these steps:\n",
    "\n",
    "1. **Load Data** üìä\n",
    "2. **Explore & Visualize** üîç\n",
    "3. **Prepare Data** üõ†Ô∏è\n",
    "4. **Split Data** ‚úÇÔ∏è\n",
    "5. **Train Model** üß†\n",
    "6. **Evaluate Performance** üìà\n",
    "7. **Make Predictions** üéØ\n",
    "\n",
    "Let's walk through this process with real examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be85d4ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üå∏ Project 1: Iris Flower Classification\n",
    "\n",
    "Let's start with the famous Iris dataset - the \"Hello World\" of machine learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd28a9f",
   "metadata": {},
   "source": [
    "### Step 1: Load and Explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Convert to DataFrame for easier exploration\n",
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "iris_df['species_name'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "print(\"üå∏ Iris Dataset Overview:\")\n",
    "print(f\"Shape: {iris_df.shape}\")\n",
    "print(f\"Features: {list(iris.feature_names)}\")\n",
    "print(f\"Target classes: {list(iris.target_names)}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87539044",
   "metadata": {},
   "source": [
    "### Step 2: Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20dcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization to understand the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Dataset Exploration', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Scatter plot: Sepal length vs Sepal width\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = iris_df['species_name'] == species\n",
    "    axes[0, 0].scatter(iris_df[mask]['sepal length (cm)'], \n",
    "                      iris_df[mask]['sepal width (cm)'], \n",
    "                      label=species, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Sepal Length (cm)')\n",
    "axes[0, 0].set_ylabel('Sepal Width (cm)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_title('Sepal Measurements')\n",
    "\n",
    "# Scatter plot: Petal length vs Petal width\n",
    "for i, species in enumerate(iris.target_names):\n",
    "    mask = iris_df['species_name'] == species\n",
    "    axes[0, 1].scatter(iris_df[mask]['petal length (cm)'], \n",
    "                      iris_df[mask]['petal width (cm)'], \n",
    "                      label=species, alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Petal Length (cm)')\n",
    "axes[0, 1].set_ylabel('Petal Width (cm)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].set_title('Petal Measurements')\n",
    "\n",
    "# Distribution of species\n",
    "species_counts = iris_df['species_name'].value_counts()\n",
    "axes[1, 0].bar(species_counts.index, species_counts.values)\n",
    "axes[1, 0].set_xlabel('Species')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Species Distribution')\n",
    "\n",
    "# Feature distributions\n",
    "iris_df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].hist(ax=axes[1, 1], bins=15)\n",
    "axes[1, 1].set_title('Feature Distributions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Observations:\")\n",
    "print(\"- Setosa flowers have smaller petals\")\n",
    "print(\"- Virginica flowers have the largest petals\")\n",
    "print(\"- Species seem separable based on petal measurements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58ed76",
   "metadata": {},
   "source": [
    "### Step 3: Prepare and Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Target: species (0=setosa, 1=versicolor, 2=virginica)\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# 80% for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nTraining set class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing set class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ddd54",
   "metadata": {},
   "source": [
    "### Step 4: Train Your First Machine Learning Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05831f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Random Forest Classifier\n",
    "print(\"üß† Training Random Forest Classifier...\")\n",
    "\n",
    "# Create the model\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    random_state=42    # For reproducible results\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Model training completed!\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "print(f\"\\nüéØ Predictions made for {len(y_test)} test samples\")\n",
    "print(f\"Actual species: {y_test}\")\n",
    "print(f\"Predicted species: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9ba8d",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb45536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"üéØ Model Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìä Detailed Performance Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = rf_classifier.feature_importances_\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(feature_names, feature_importance)\n",
    "plt.title('Feature Importance in Iris Classification', fontweight='bold')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, importance in zip(bars, feature_importance):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{importance:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Most important feature: {feature_names[np.argmax(feature_importance)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b095d767",
   "metadata": {},
   "source": [
    "### Step 6: Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict the species for some new flower measurements\n",
    "new_flowers = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Looks like setosa\n",
    "    [6.2, 2.8, 4.8, 1.8],  # Looks like versicolor/virginica\n",
    "    [7.2, 3.0, 5.8, 1.6]   # Looks like virginica\n",
    "])\n",
    "\n",
    "predictions = rf_classifier.predict(new_flowers)\n",
    "probabilities = rf_classifier.predict_proba(new_flowers)\n",
    "\n",
    "print(\"üîÆ Predictions for New Flowers:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for i, (flower, pred, prob) in enumerate(zip(new_flowers, predictions, probabilities)):\n",
    "    predicted_species = iris.target_names[pred]\n",
    "    confidence = prob[pred] * 100\n",
    "    \n",
    "    print(f\"\\nFlower {i+1}: {flower}\")\n",
    "    print(f\"Predicted species: {predicted_species} (confidence: {confidence:.1f}%)\")\n",
    "    \n",
    "    # Show all probabilities\n",
    "    print(\"All probabilities:\")\n",
    "    for j, species in enumerate(iris.target_names):\n",
    "        print(f\"  {species}: {prob[j]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89bd099",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üè† Project 2: House Price Prediction (Regression)\n",
    "\n",
    "Now let's try a regression problem - predicting house prices!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02861698",
   "metadata": {},
   "source": [
    "### Load and Explore the Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# Convert to DataFrame\n",
    "boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_df['price'] = boston.target\n",
    "\n",
    "print(\"üè† Boston Housing Dataset Overview:\")\n",
    "print(f\"Shape: {boston_df.shape}\")\n",
    "print(f\"Target: House prices (in $1000s)\")\n",
    "\n",
    "print(\"\\nDataset description:\")\n",
    "print(boston.DESCR[:500] + \"...\")\n",
    "\n",
    "# Show basic statistics\n",
    "print(\"\\nüìä Price Statistics:\")\n",
    "print(f\"Mean price: ${boston_df['price'].mean():.1f}k\")\n",
    "print(f\"Median price: ${boston_df['price'].median():.1f}k\")\n",
    "print(f\"Price range: ${boston_df['price'].min():.1f}k - ${boston_df['price'].max():.1f}k\")\n",
    "\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a10c56",
   "metadata": {},
   "source": [
    "### Visualize Key Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize relationships between features and house prices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('House Price Relationships', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Price distribution\n",
    "axes[0, 0].hist(boston_df['price'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Price ($1000s)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Price Distribution')\n",
    "\n",
    "# Number of rooms vs Price\n",
    "axes[0, 1].scatter(boston_df['RM'], boston_df['price'], alpha=0.6)\n",
    "axes[0, 1].set_xlabel('Average Number of Rooms')\n",
    "axes[0, 1].set_ylabel('Price ($1000s)')\n",
    "axes[0, 1].set_title('Rooms vs Price')\n",
    "\n",
    "# Crime rate vs Price\n",
    "axes[0, 2].scatter(boston_df['CRIM'], boston_df['price'], alpha=0.6)\n",
    "axes[0, 2].set_xlabel('Crime Rate')\n",
    "axes[0, 2].set_ylabel('Price ($1000s)')\n",
    "axes[0, 2].set_title('Crime Rate vs Price')\n",
    "\n",
    "# Distance to employment centers vs Price\n",
    "axes[1, 0].scatter(boston_df['DIS'], boston_df['price'], alpha=0.6)\n",
    "axes[1, 0].set_xlabel('Distance to Employment Centers')\n",
    "axes[1, 0].set_ylabel('Price ($1000s)')\n",
    "axes[1, 0].set_title('Distance vs Price')\n",
    "\n",
    "# Pupil-teacher ratio vs Price\n",
    "axes[1, 1].scatter(boston_df['PTRATIO'], boston_df['price'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Pupil-Teacher Ratio')\n",
    "axes[1, 1].set_ylabel('Price ($1000s)')\n",
    "axes[1, 1].set_title('School Quality vs Price')\n",
    "\n",
    "# Correlation heatmap of top features\n",
    "correlation_features = ['price', 'RM', 'LSTAT', 'PTRATIO', 'DIS', 'CRIM']\n",
    "corr_matrix = boston_df[correlation_features].corr()\n",
    "im = axes[1, 2].imshow(corr_matrix, cmap='RdBu', vmin=-1, vmax=1)\n",
    "axes[1, 2].set_xticks(range(len(correlation_features)))\n",
    "axes[1, 2].set_yticks(range(len(correlation_features)))\n",
    "axes[1, 2].set_xticklabels(correlation_features, rotation=45)\n",
    "axes[1, 2].set_yticklabels(correlation_features)\n",
    "axes[1, 2].set_title('Feature Correlations')\n",
    "\n",
    "# Add correlation values to heatmap\n",
    "for i in range(len(correlation_features)):\n",
    "    for j in range(len(correlation_features)):\n",
    "        axes[1, 2].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}', \n",
    "                       ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Key Insights:\")\n",
    "print(\"- More rooms = higher prices\")\n",
    "print(\"- Higher crime rate = lower prices\")\n",
    "print(\"- Better schools (lower pupil-teacher ratio) = higher prices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad347e9",
   "metadata": {},
   "source": [
    "### Train Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a69375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_boston = boston.data\n",
    "y_boston = boston.target\n",
    "\n",
    "# Split the data\n",
    "X_train_boston, X_test_boston, y_train_boston, y_test_boston = train_test_split(\n",
    "    X_boston, y_boston, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üè† Training set: {X_train_boston.shape[0]} houses\")\n",
    "print(f\"üè† Testing set: {X_test_boston.shape[0]} houses\")\n",
    "\n",
    "# Train two different models\n",
    "print(\"\\nüß† Training models...\")\n",
    "\n",
    "# 1. Linear Regression\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train_boston, y_train_boston)\n",
    "\n",
    "# 2. Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train_boston, y_train_boston)\n",
    "\n",
    "print(\"‚úÖ Both models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9fc3e",
   "metadata": {},
   "source": [
    "### Compare Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with both models\n",
    "y_pred_linear = linear_reg.predict(X_test_boston)\n",
    "y_pred_rf = rf_regressor.predict(X_test_boston)\n",
    "\n",
    "# Calculate Mean Squared Error for both models\n",
    "mse_linear = mean_squared_error(y_test_boston, y_pred_linear)\n",
    "mse_rf = mean_squared_error(y_test_boston, y_pred_rf)\n",
    "\n",
    "# Calculate R¬≤ score (coefficient of determination)\n",
    "r2_linear = linear_reg.score(X_test_boston, y_test_boston)\n",
    "r2_rf = rf_regressor.score(X_test_boston, y_test_boston)\n",
    "\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Linear Regression:\")\n",
    "print(f\"  Mean Squared Error: {mse_linear:.2f}\")\n",
    "print(f\"  R¬≤ Score: {r2_linear:.3f} ({r2_linear*100:.1f}% variance explained)\")\n",
    "print(f\"  RMSE: ${np.sqrt(mse_linear):.2f}k\")\n",
    "\n",
    "print(f\"\\nRandom Forest:\")\n",
    "print(f\"  Mean Squared Error: {mse_rf:.2f}\")\n",
    "print(f\"  R¬≤ Score: {r2_rf:.3f} ({r2_rf*100:.1f}% variance explained)\")\n",
    "print(f\"  RMSE: ${np.sqrt(mse_rf):.2f}k\")\n",
    "\n",
    "# Visualize predictions vs actual values\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Linear Regression predictions\n",
    "axes[0].scatter(y_test_boston, y_pred_linear, alpha=0.6)\n",
    "axes[0].plot([y_test_boston.min(), y_test_boston.max()], \n",
    "            [y_test_boston.min(), y_test_boston.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price ($1000s)')\n",
    "axes[0].set_ylabel('Predicted Price ($1000s)')\n",
    "axes[0].set_title(f'Linear Regression\\nR¬≤ = {r2_linear:.3f}')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest predictions\n",
    "axes[1].scatter(y_test_boston, y_pred_rf, alpha=0.6)\n",
    "axes[1].plot([y_test_boston.min(), y_test_boston.max()], \n",
    "            [y_test_boston.min(), y_test_boston.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price ($1000s)')\n",
    "axes[1].set_ylabel('Predicted Price ($1000s)')\n",
    "axes[1].set_title(f'Random Forest\\nR¬≤ = {r2_rf:.3f}')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "winner = \"Random Forest\" if r2_rf > r2_linear else \"Linear Regression\"\n",
    "print(f\"\\nüèÜ Winner: {winner} performs better on this dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d684c",
   "metadata": {},
   "source": [
    "### Feature Importance in House Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5268010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from Random Forest\n",
    "feature_importance_boston = rf_regressor.feature_importances_\n",
    "feature_names_boston = boston.feature_names\n",
    "\n",
    "# Create a DataFrame for easier handling\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_names_boston,\n",
    "    'importance': feature_importance_boston\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance in House Price Prediction', fontweight='bold')\n",
    "plt.gca().invert_yaxis()  # Highest importance at top\n",
    "\n",
    "# Add value labels\n",
    "for bar, importance in zip(bars, importance_df['importance']):\n",
    "    plt.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, \n",
    "             f'{importance:.3f}', ha='left', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üè† Most Important Features for House Prices:\")\n",
    "for i, (_, row) in enumerate(importance_df.head().iterrows()):\n",
    "    print(f\"{i+1}. {row['feature']}: {row['importance']:.3f}\")\n",
    "\n",
    "# Feature explanations\n",
    "feature_explanations = {\n",
    "    'LSTAT': 'Lower status of population (%)',\n",
    "    'RM': 'Average number of rooms per dwelling',\n",
    "    'PTRATIO': 'Pupil-teacher ratio by town',\n",
    "    'DIS': 'Distances to employment centers',\n",
    "    'NOX': 'Nitric oxides concentration',\n",
    "    'CRIM': 'Per capita crime rate',\n",
    "    'TAX': 'Property tax rate',\n",
    "    'AGE': 'Proportion of owner-occupied units built prior to 1940'\n",
    "}\n",
    "\n",
    "print(\"\\nüìñ Feature Explanations:\")\n",
    "for feature in importance_df['feature'].head(5):\n",
    "    if feature in feature_explanations:\n",
    "        print(f\"‚Ä¢ {feature}: {feature_explanations[feature]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312ac11",
   "metadata": {},
   "source": [
    "### Make Predictions for New Houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed53c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's predict prices for some example houses\n",
    "# Using average values but changing key features\n",
    "average_features = X_boston.mean(axis=0)\n",
    "\n",
    "# Create 3 different house profiles\n",
    "house_profiles = []\n",
    "\n",
    "# House 1: Luxury house (more rooms, low crime, good schools)\n",
    "luxury_house = average_features.copy()\n",
    "luxury_house[5] = 8.0    # RM: 8 rooms (vs avg 6.3)\n",
    "luxury_house[0] = 0.5    # CRIM: Low crime (vs avg 3.6)\n",
    "luxury_house[10] = 15.0  # PTRATIO: Good schools (vs avg 18.5)\n",
    "house_profiles.append((\"Luxury House\", luxury_house))\n",
    "\n",
    "# House 2: Average house\n",
    "house_profiles.append((\"Average House\", average_features))\n",
    "\n",
    "# House 3: Budget house (fewer rooms, higher crime, worse schools)\n",
    "budget_house = average_features.copy()\n",
    "budget_house[5] = 4.5    # RM: 4.5 rooms\n",
    "budget_house[0] = 8.0    # CRIM: Higher crime\n",
    "budget_house[10] = 22.0  # PTRATIO: Worse schools\n",
    "house_profiles.append((\"Budget House\", budget_house))\n",
    "\n",
    "print(\"üè† House Price Predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, features in house_profiles:\n",
    "    # Reshape for prediction (sklearn expects 2D array)\n",
    "    features_2d = features.reshape(1, -1)\n",
    "    \n",
    "    # Predict with both models\n",
    "    price_linear = linear_reg.predict(features_2d)[0]\n",
    "    price_rf = rf_regressor.predict(features_2d)[0]\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Rooms: {features[5]:.1f}\")\n",
    "    print(f\"  Crime Rate: {features[0]:.1f}\")\n",
    "    print(f\"  Pupil-Teacher Ratio: {features[10]:.1f}\")\n",
    "    print(f\"  Predicted Price (Linear): ${price_linear:.1f}k\")\n",
    "    print(f\"  Predicted Price (Random Forest): ${price_rf:.1f}k\")\n",
    "    print(f\"  Average Prediction: ${(price_linear + price_rf)/2:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74c99df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Key Machine Learning Concepts You've Learned\n",
    "\n",
    "### **Classification vs Regression**\n",
    "- **Classification**: Predicting categories (Iris species)\n",
    "- **Regression**: Predicting continuous values (house prices)\n",
    "\n",
    "### **The ML Workflow**\n",
    "1. **Data Loading**: `load_iris()`, `load_boston()`\n",
    "2. **Data Splitting**: `train_test_split()`\n",
    "3. **Model Training**: `.fit()`\n",
    "4. **Predictions**: `.predict()`\n",
    "5. **Evaluation**: `accuracy_score()`, `mean_squared_error()`\n",
    "\n",
    "### **Model Types**\n",
    "- **Random Forest**: Combines many decision trees (usually performs well)\n",
    "- **Linear Regression**: Simple, interpretable, good baseline\n",
    "- **Logistic Regression**: Linear model for classification\n",
    "\n",
    "### **Evaluation Metrics**\n",
    "- **Classification**: Accuracy, Precision, Recall, F1-score\n",
    "- **Regression**: Mean Squared Error (MSE), R¬≤ score, RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb608c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîß Common ML Errors and How to Fix Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4773eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common ML Mistakes and Solutions\n",
    "\n",
    "print(\"üö® Common Machine Learning Errors and Solutions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. Data Shape Errors:\")\n",
    "print(\"   Problem: ValueError: Expected 2D array, got 1D array\")\n",
    "print(\"   Solution: Use .reshape(-1, 1) or .reshape(1, -1)\")\n",
    "\n",
    "# Example of correct reshaping\n",
    "single_sample = X_test[0]  # This is 1D\n",
    "print(f\"   1D shape: {single_sample.shape}\")\n",
    "single_sample_2d = single_sample.reshape(1, -1)  # Make it 2D\n",
    "print(f\"   2D shape: {single_sample_2d.shape}\")\n",
    "\n",
    "print(\"\\n2. Data Leakage:\")\n",
    "print(\"   Problem: Accidentally using future information to predict the past\")\n",
    "print(\"   Solution: Always split data BEFORE any preprocessing\")\n",
    "\n",
    "print(\"\\n3. Overfitting:\")\n",
    "print(\"   Problem: Model performs great on training data, poor on test data\")\n",
    "print(\"   Solution: Use cross-validation, simpler models, more data\")\n",
    "\n",
    "print(\"\\n4. Underfitting:\")\n",
    "print(\"   Problem: Model performs poorly on both training and test data\")\n",
    "print(\"   Solution: Use more complex models, more features, less regularization\")\n",
    "\n",
    "print(\"\\n5. Wrong Metric:\")\n",
    "print(\"   Problem: Using accuracy for imbalanced datasets\")\n",
    "print(\"   Solution: Use precision, recall, F1-score, or ROC-AUC\")\n",
    "\n",
    "print(\"\\nüí° Best Practices:\")\n",
    "print(\"   ‚úÖ Always set random_state for reproducible results\")\n",
    "print(\"   ‚úÖ Use stratify=y for balanced train/test splits\")\n",
    "print(\"   ‚úÖ Scale/normalize features when needed\")\n",
    "print(\"   ‚úÖ Visualize your data before modeling\")\n",
    "print(\"   ‚úÖ Start with simple models, then increase complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608622c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è Practice Challenge: Build Your Own Model\n",
    "\n",
    "Now it's your turn! Try to solve this challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f2853",
   "metadata": {},
   "source": [
    "### Challenge: Wine Quality Prediction\n",
    "\n",
    "Your task: Predict wine quality using the famous wine dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94f8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge: Wine Quality Prediction\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_df['wine_class'] = wine.target\n",
    "wine_df['wine_name'] = wine_df['wine_class'].map({0: 'class_0', 1: 'class_1', 2: 'class_2'})\n",
    "\n",
    "print(\"üç∑ Wine Dataset Challenge:\")\n",
    "print(f\"Shape: {wine_df.shape}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "print(f\"Features: {len(wine.feature_names)} chemical properties\")\n",
    "\n",
    "print(\"\\nüéØ Your Challenge:\")\n",
    "print(\"1. Explore the wine dataset\")\n",
    "print(\"2. Split the data into train/test sets\")\n",
    "print(\"3. Train a classifier (try different models!)\")\n",
    "print(\"4. Evaluate performance\")\n",
    "print(\"5. Find the most important features\")\n",
    "\n",
    "print(\"\\nüí™ Stretch Goals:\")\n",
    "print(\"- Compare multiple models\")\n",
    "print(\"- Create visualizations\")\n",
    "print(\"- Try feature selection\")\n",
    "\n",
    "# Show first few rows to get started\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f52372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE - Try the wine classification challenge!\n",
    "# Start by exploring the data, then build your model\n",
    "\n",
    "# Hint: You can copy and modify code from the Iris example above\n",
    "\n",
    "# Step 1: Explore the data\n",
    "# print(wine_df.describe())\n",
    "\n",
    "# Step 2: Prepare and split the data\n",
    "# X_wine = wine.data\n",
    "# y_wine = wine.target\n",
    "# X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(...)\n",
    "\n",
    "# Step 3: Train a model\n",
    "# model = RandomForestClassifier(...)\n",
    "# model.fit(...)\n",
    "\n",
    "# Step 4: Evaluate\n",
    "# predictions = model.predict(...)\n",
    "# accuracy = accuracy_score(...)\n",
    "\n",
    "# Write your solution below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a7bbd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Self-Assessment Checklist\n",
    "\n",
    "Before moving forward, make sure you can:\n",
    "\n",
    "**Machine Learning Fundamentals:**\n",
    "- [ ] Understand the difference between classification and regression\n",
    "- [ ] Follow the ML workflow: load ‚Üí explore ‚Üí split ‚Üí train ‚Üí evaluate ‚Üí predict\n",
    "- [ ] Use `train_test_split()` to create training and testing sets\n",
    "- [ ] Explain why we split data (to avoid overfitting)\n",
    "\n",
    "**Model Training:**\n",
    "- [ ] Import and use scikit-learn models\n",
    "- [ ] Train models with `.fit(X_train, y_train)`\n",
    "- [ ] Make predictions with `.predict(X_test)`\n",
    "- [ ] Get prediction probabilities with `.predict_proba()`\n",
    "\n",
    "**Model Evaluation:**\n",
    "- [ ] Calculate accuracy for classification problems\n",
    "- [ ] Calculate MSE and R¬≤ for regression problems\n",
    "- [ ] Interpret classification reports\n",
    "- [ ] Understand feature importance\n",
    "\n",
    "**Practical Skills:**\n",
    "- [ ] Debug common shape errors (1D vs 2D arrays)\n",
    "- [ ] Visualize model predictions vs actual values\n",
    "- [ ] Compare different models\n",
    "- [ ] Make predictions on new data\n",
    "\n",
    "**Data Science Connection:**\n",
    "- [ ] Recognize real-world ML applications\n",
    "- [ ] Understand when to use different models\n",
    "- [ ] Know how to interpret results for business decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cfa2e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üöÄ What's Next in Your Data Science Journey?\n",
    "\n",
    "Congratulations! You've built your first machine learning models. Here's what to explore next:\n",
    "\n",
    "### **Immediate Next Steps:**\n",
    "- **Cross-Validation**: Learn to validate models properly\n",
    "- **Feature Engineering**: Create better features from raw data\n",
    "- **Model Tuning**: Optimize hyperparameters for better performance\n",
    "- **Pipeline Creation**: Automate your ML workflow\n",
    "\n",
    "### **Advanced Topics to Explore:**\n",
    "- **Ensemble Methods**: Combine multiple models\n",
    "- **Deep Learning**: Neural networks with TensorFlow/PyTorch\n",
    "- **Natural Language Processing**: Work with text data\n",
    "- **Computer Vision**: Work with image data\n",
    "- **Time Series**: Predict future values\n",
    "\n",
    "### **Real-World Applications:**\n",
    "- **Business Analytics**: Customer segmentation, churn prediction\n",
    "- **Finance**: Fraud detection, algorithmic trading\n",
    "- **Healthcare**: Medical diagnosis, drug discovery\n",
    "- **Technology**: Recommendation systems, search algorithms\n",
    "\n",
    "### **Tools to Master:**\n",
    "- **Advanced Scikit-learn**: More algorithms and techniques\n",
    "- **XGBoost/LightGBM**: State-of-the-art gradient boosting\n",
    "- **TensorFlow/PyTorch**: Deep learning frameworks\n",
    "- **MLflow/Kubeflow**: ML experiment tracking and deployment\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've completed your introduction to machine learning with scikit-learn! You now have the foundation to:\n",
    "\n",
    "‚úÖ **Build classification models** to predict categories  \n",
    "‚úÖ **Build regression models** to predict continuous values  \n",
    "‚úÖ **Evaluate model performance** using appropriate metrics  \n",
    "‚úÖ **Understand feature importance** in your models  \n",
    "‚úÖ **Apply ML to real-world problems** with confidence  \n",
    "\n",
    "**Remember:** Every data scientist started where you are now. The key is to keep practicing with real datasets and gradually tackle more complex problems.\n",
    "\n",
    "**What will you predict next? üîÆ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
